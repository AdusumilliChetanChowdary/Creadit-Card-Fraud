# -*- coding: utf-8 -*-
"""credit-card-fraud-detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A8Bh3FMfwGttVg41GcssW5wYMz2wnXO7
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import numpy as np
import pandas as pd

dataset_path1 = '/kaggle/input/creditcardfraud/creditcard.csv'# Update with your dataset path
df = pd.read_csv(dataset_path1)

print(df)

fraud_data = df[df['Class'] == 1]
not_fraud_data = df[df['Class'] == 0]

fraud_size = 400

# Randomly sample a subset of fraud data
fraud_data_sampled = fraud_data.sample(n=fraud_size, random_state=42)

nonfraud_size = 14000
# Randomly sample a subset of not fraud data
not_fraud_data_sampled = not_fraud_data.sample(n=nonfraud_size, random_state=42)

# Concatenate fraud and sampled not fraud data to create a partially balanced dataset
balanced_df = pd.concat([fraud_data_sampled, not_fraud_data_sampled])

# Shuffle the dataset
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

print(balanced_df)

total_null_values = balanced_df.isna().sum()
print("Total Null Values:", total_null_values)

total_duplicates = balanced_df.duplicated().sum()
print("Total Duplicates:", total_duplicates)

balanced_df.drop_duplicates(inplace=True)
print(balanced_df)

description=balanced_df.describe()
print(description)

class_counts = balanced_df['Class'].value_counts()
print("Class Distribution:")
print(class_counts)

X = balanced_df.drop(["Class"], axis=1)
Y = balanced_df["Class"]

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

dt_model = DecisionTreeClassifier(random_state=3)

# Train the model on the training data
dt_model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred = dt_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(Y_test, Y_pred)
classification_report_result = classification_report(Y_test, Y_pred)
confusion_matrix_result = confusion_matrix(Y_test, Y_pred)

# Print the results
print(f"Accuracy Score: {accuracy}")
print("\nClassification Report:")
print(classification_report_result)
print("\nConfusion Matrix:")
print(confusion_matrix_result)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

svm_model=SVC()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm_model.fit(X_train_scaled, Y_train)

pred = svm_model.predict(X_test_scaled)

print("Accuracy Score: ", accuracy_score(Y_test, pred))
print("Confusion Matrix:\n", confusion_matrix(Y_test, pred))
print("Classification Report:\n", classification_report(Y_test, pred))

import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(Y_test, pred)
plt.figure(figsize=(4, 2))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=svm_model.classes_, yticklabels=svm_model.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

dataset_path2 = '/kaggle/input/credit-card-fraud-detection-dataset-2023/creditcard_2023.csv'# Update with your dataset path
df = pd.read_csv(dataset_path2)

print(df)

# Separate data into fraud and not fraud
fraud_data = df[df['Class'] == 1]
not_fraud_data = df[df['Class'] == 0]

fraud_size = 4000

# Randomly sample a subset of fraud data
fraud_data_sampled = fraud_data.sample(n=fraud_size, random_state=42)

nonfraud_size = 11000
# Randomly sample a subset of not fraud data
not_fraud_data_sampled = not_fraud_data.sample(n=nonfraud_size, random_state=42)

# Concatenate fraud and sampled not fraud data to create a partially balanced dataset
balanced_df = pd.concat([fraud_data_sampled, not_fraud_data_sampled])

your_feature = 'id'
feature_series = df[your_feature]

# Reset the index for the feature
feature_series_reset = feature_series.reset_index(drop=True)

# Shuffle the dataset
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
balanced_df[your_feature] = feature_series_reset

print(balanced_df)

balanced_df.drop(your_feature, axis=1, inplace=True)
print(balanced_df)

total_null_values = balanced_df.isna().sum()
print("Total Null Values:", total_null_values)

total_duplicates = balanced_df.duplicated().sum()
print("Total Duplicates:", total_duplicates)

balanced_df.drop_duplicates(inplace=True)
print(balanced_df)

description=balanced_df.describe()
print(description)

class_counts = balanced_df['Class'].value_counts()
print("Class Distribution:")
print(class_counts)

X = balanced_df.drop(["Class"], axis=1)
Y = balanced_df["Class"]

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

dt_model = DecisionTreeClassifier(random_state=3)

# Train the model on the training data
dt_model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred = dt_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(Y_test, Y_pred)
classification_report_result = classification_report(Y_test, Y_pred)
confusion_matrix_result = confusion_matrix(Y_test, Y_pred)

# Print the results
print(f"Accuracy Score: {accuracy}")
print("\nClassification Report:")
print(classification_report_result)
print("\nConfusion Matrix:")
print(confusion_matrix_result)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

svm_model=SVC()

from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm_model.fit(X_train_scaled, Y_train)

pred = svm_model.predict(X_test_scaled)

print("Accuracy Score: ", accuracy_score(Y_test, pred))
print("Confusion Matrix:\n", confusion_matrix(Y_test, pred))
print("Classification Report:\n", classification_report(Y_test, pred))

import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(Y_test, pred)
plt.figure(figsize=(4, 2))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=svm_model.classes_, yticklabels=svm_model.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

dataset_path3 = '/kaggle/input/credit-card-fraud/card_transdata.csv'# Update with your dataset path
df = pd.read_csv(dataset_path3)

print(df)

fraud_data = df[df['fraud'] == 1.0]
not_fraud_data = df[df['fraud'] == 0.0]

fraud_size = 3000

# Randomly sample a subset of fraud data
fraud_data_sampled = fraud_data.sample(n=fraud_size, random_state=42)

nonfraud_size = 10500
# Randomly sample a subset of not fraud data
not_fraud_data_sampled = not_fraud_data.sample(n=nonfraud_size, random_state=42)

# Concatenate fraud and sampled not fraud data to create a partially balanced dataset
balanced_df = pd.concat([fraud_data_sampled, not_fraud_data_sampled])

# Shuffle the dataset
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Shuffle the dataset
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

print(balanced_df)

total_null_values = balanced_df.isna().sum()
print("Total Null Values:", total_null_values)

total_duplicates = balanced_df.duplicated().sum()
print("Total Duplicates:", total_duplicates)

balanced_df.drop_duplicates(inplace=True)
print(balanced_df)

description=balanced_df.describe()
print(description)

class_counts = balanced_df['fraud'].value_counts()
print("Class Distribution:")
print(class_counts)

X = balanced_df.drop(["fraud"], axis=1)
Y = balanced_df["fraud"]

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

dt_model = DecisionTreeClassifier(random_state=3)

# Train the model on the training data
dt_model.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred = dt_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(Y_test, Y_pred)
classification_report_result = classification_report(Y_test, Y_pred)
confusion_matrix_result = confusion_matrix(Y_test, Y_pred)

# Print the results
print(f"Accuracy Score: {accuracy}")
print("\nClassification Report:")
print(classification_report_result)
print("\nConfusion Matrix:")
print(confusion_matrix_result)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

svm_model=SVC()

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm_model.fit(X_train_scaled, Y_train)

pred = svm_model.predict(X_test_scaled)

print("Accuracy Score: ", accuracy_score(Y_test, pred))
print("Confusion Matrix:\n", confusion_matrix(Y_test, pred))
print("Classification Report:\n", classification_report(Y_test, pred))

import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(Y_test, pred)
plt.figure(figsize=(4, 2))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=svm_model.classes_, yticklabels=svm_model.classes_)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()